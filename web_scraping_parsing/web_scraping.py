# -*- coding: utf-8 -*-
""""Web-scraping.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OV-GYeaVLMjOo7J2LRa0gUOi1MFhNd1Z

Web-scraping / Python

 1.  web-scraping  сайта с использованием библиотеки BeautifulSoup  

2.  HTML разбор текста
"""

import requests
import json
from bs4 import BeautifulSoup

# сайт ivi - Вкладка : Новинки уже вышедшие и доступные для просмотра

l = 'https://www.ivi.ru/new/available'
movie = requests.get(l)

movie.status_code
movie.text

soup=BeautifulSoup(movie.text, 'lxml')
print(soup.prettify())

movie1 = soup.find_all('li', attrs = {'class' : 'noveltyPromo noveltyPromo_type_compact noveltyPromo_virtual'})

movie1[0]

# получаем Название фильма (name)
movie1[0].find('div', attrs= {'class': 'noveltyPromo__headerInfoTextTitle'}).get_text()

# Дата (Year/t/range)
movie1[0].find('div', attrs= {'class': 'noveltyPromo__meta'}).get_text()

# получаем оценку зрителей на фильм (rating)
movie1[0].find('div', attrs= {'class': ['nbl-textBadge nbl-textBadge_size_huxa nbl-textBadge_style_dan nbl-textBadge_isShadowEnabled_1 noveltyPromoTrailerBlock__nbl-textBadge noveltyPromoTrailerBlock__ratingTextBadge',
                                        'nbl-textBadge__text']}).get_text()

# Описание (description)
movie1[0].find('div', attrs= {'class': 'clause__text-inner hidden-children'}).get_text()

# получаем ссылку на фильм (url)
'https://www.ivi.ru/'+ movie1[0].find('a').get('href')

list_movie =list ()
dict_movie = dict.fromkeys([('Name', 'Year/t/range', 'Rating','Description', 'url')], 0)

#цикл на основе словаря (вписываем в лист все элементы на 1 странице)
for item in movie1:
  dict_movie['Name'] = item.find('div', attrs= {'class': 'noveltyPromo__headerInfoTextTitle'}).get_text()
  dict_movie['Year/t/range'] = item.find('div', attrs= {'class': 'noveltyPromo__meta'}).get_text()
  dict_movie['Rating'] = item.find('div', attrs= {'class': ['nbl-textBadge nbl-textBadge_size_huxa nbl-textBadge_style_dan nbl-textBadge_isShadowEnabled_1 noveltyPromoTrailerBlock__nbl-textBadge noveltyPromoTrailerBlock__ratingTextBadge',
                                        'nbl-textBadge__text']}).get_text()
  dict_movie['Description'] = item.find('div', attrs= {'class': 'clause__text-inner hidden-children'}).get_text()
  dict_movie['url'] = 'https://www.ivi.ru/' + item.find('a').get('href')
  list_movie.append(dict_movie.copy())

import pandas as pd

list_movie

pd.DataFrame(list_movie)

pd.DataFrame(list_movie)

# Проверка:  выдает ли правильно 54 книгу, которая должна находиться на Page2

movie1[54]

movie1[54].find('div', attrs= {'class': 'noveltyPromo__headerInfoTextTitle'}).get_text()

movie1[54].find('div', attrs= {'class': 'noveltyPromo__meta'}).get_text()

movie1[54].find('div', attrs= {'class': ['nbl-textBadge nbl-textBadge_size_huxa nbl-textBadge_style_dan nbl-textBadge_isShadowEnabled_1 noveltyPromoTrailerBlock__nbl-textBadge noveltyPromoTrailerBlock__ratingTextBadge',
                                        'nbl-textBadge__text']}).get_text()

movie1[54].find('div', attrs= {'class': 'clause__text-inner hidden-children'}).get_text()

"""Парсим сайт mybook.ru"""

# Топ Апреля 2023
link = 'https://mybook.ru/sets/12346-top-aprelya-2023/'
book = requests.get(link)

book.status_code
book.text

soup_1 =BeautifulSoup(book.text, 'lxml')
print(soup_1.prettify())

book1 = soup_1.find_all('div', attrs = {'class' : 'e4xwgl-0 iJwsmp'})
book1[0]

# получаем Название книги (Name_1)
book1[0].find('p', attrs= {'lnjchu-1 hhskLb'}).get_text()

# Автор (Author)
book1[0].find('div', attrs= {'class': 'dey4wx-1 jVKkXg'}).get_text()

# получаем оценку зрителей на книгу (Rating_1)
book1[0].find('span', attrs= {'class': 'sc-1vvnv6o-3 hTOIIb'}).get_text()

# Описание (Description_1)
book1[0].find('p', attrs= {'lnjchu-1 dPgoNf'}).get_text()

# получаем ссылку на book (url_1)
'https://mybook.ru/sets/12346-top-aprelya-2023/'+ book1[0].find('a').get('href')

list_book =list ()
dict_book = dict.fromkeys([('Name_1', 'Author', 'Rating_1','Description_1', 'url_1')], 2023)

#Делаем цикл на основе словаря и вписываем в лист все элементы на 1 странице
for item in book1:
  dict_book['Name_1'] = item.find('p', attrs= {'lnjchu-1 hhskLb'}).get_text()
  dict_book['Author'] = item.find('div', attrs= {'class': 'dey4wx-1 jVKkXg'}).get_text()
  dict_book['Rating_1'] = item.find('span', attrs= {'class': 'sc-1vvnv6o-3 hTOIIb'}).get_text()
  dict_book['Description_1'] = item.find('p', attrs= {'lnjchu-1 dPgoNf'}).get_text()
  dict_book['url_1'] = 'https://mybook.ru/sets/12346-top-aprelya-2023/'+ book1[0].find('a').get('href')
  list_book.append(dict_book.copy())

# создаем датафрейэм из книг на 1 странице
pd.DataFrame(list_book)

# создаем датафрейэм из книг на всех страницах
for page_number_1 in range(1,30):
  result_1 = requests.get('https://mybook.ru/sets/12346-top-aprelya-2023/?page=' + str(page_number))
  soup_1 = BeautifulSoup(result_1.text , 'lxml')
  book1 = soup_1.find_all('div', attrs = {'class' : 'e4xwgl-0 iJwsmp'})
  for item in book1:
    dict_book['Name_1'] = item.find('p', attrs= {'lnjchu-1 hhskLb'}).get_text()
    dict_book['Author'] = item.find('div', attrs= {'class': 'dey4wx-1 jVKkXg'}).get_text()
    dict_book['Rating_1'] = item.find('span', attrs= {'class': 'sc-1vvnv6o-3 hTOIIb'}).get_text()
    dict_book['Description_1'] = item.find('p', attrs= {'lnjchu-1 dPgoNf'}).get_text()
    dict_book['url_1'] = 'https://mybook.ru/sets/12346-top-aprelya-2023/'+ book1[0].find('a').get('href')
    list_book.append(dict_book.copy())

pd.DataFrame(list_book)